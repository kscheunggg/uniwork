{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acb79125-0920-46e8-8d01-38bada2b3e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: Positive-to-Negative Ratio = 1.01\n",
      "Testing Set: Positive-to-Negative Ratio = 0.98\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_path = \"/Users/shun/Desktop/IERG4080/aclImdb\"\n",
    "\n",
    "#example 1\n",
    "def read_reviews(directory):\n",
    "    reviews = []\n",
    "    for filename in os.listdir(directory):\n",
    "        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "            review = file.read()\n",
    "            reviews.append(review)\n",
    "    return reviews\n",
    "\n",
    "train_positive_reviews = read_reviews(os.path.join(dataset_path,\"train\", \"pos\"))\n",
    "train_negative_reviews = read_reviews(os.path.join(dataset_path,\"train\", \"neg\"))\n",
    "\n",
    "test_positive_reviews = read_reviews(os.path.join(dataset_path, \"test\", \"pos\"))\n",
    "test_negative_reviews = read_reviews(os.path.join(dataset_path, \"test\", \"neg\"))\n",
    "\n",
    "# train_positive_reviews = pd.DataFrame(train_positive_reviews)\n",
    "# train_positive_reviews.insert(1,\"1\",\"pos\")\n",
    "\n",
    "# train_negative_reviews = pd.DataFrame(train_negative_reviews)\n",
    "# train_negative_reviews.insert(1,\"1\",\"neg\")\n",
    "\n",
    "# test_positive_reviews = pd.DataFrame(test_positive_reviews)\n",
    "# test_positive_reviews.insert(1,\"1\",\"pos\")\n",
    "\n",
    "# test_negative_reviews = pd.DataFrame(test_negative_reviews)\n",
    "# test_negative_reviews.insert(1,\"1\",\"neg\")\n",
    "\n",
    "train_data = train_positive_reviews + train_negative_reviews\n",
    "test_data = test_positive_reviews + test_negative_reviews\n",
    "full_data = train_data + test_data\n",
    "\n",
    "#exmaple 2\n",
    "# Initialize empty lists for reviews and labels\n",
    "reviews = []\n",
    "labels = []\n",
    "\n",
    "# Read the raw training data\n",
    "train_pos_dir = os.path.join(dataset_path, \"train\", \"pos\")\n",
    "train_neg_dir = os.path.join(dataset_path, \"train\", \"neg\")\n",
    "\n",
    "for filename in os.listdir(train_pos_dir):\n",
    "    with open(os.path.join(train_pos_dir, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "        review = file.read()\n",
    "        reviews.append(review)\n",
    "        labels.append(\"pos\")\n",
    "        \n",
    "for filename in os. listdir(train_neg_dir):\n",
    "    with open(os.path.join(train_neg_dir, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "        review = file.read()\n",
    "        reviews.append (review)\n",
    "        labels.append(\"neg\")\n",
    "\n",
    "# Read the raw test data\n",
    "test_pos_dir = os.path.join(dataset_path, \"test\", \"pos\")\n",
    "test_neg_dir = os.path.join(dataset_path, \"test\", \"neg\")\n",
    "\n",
    "for filename in os.listdir(test_pos_dir):\n",
    "    with open(os.path.join(test_pos_dir, filename), \"r\" , encoding=\"utf-8\") as file:\n",
    "        review = file.read()\n",
    "        reviews.append(review)\n",
    "        labels.append(\"pos\")\n",
    "        \n",
    "for filename in os.listdir(test_neg_dir):\n",
    "    with open(os.path.join(test_neg_dir, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "        review = file.read()\n",
    "        reviews.append(review)\n",
    "        labels.append(\"neg\")\n",
    "\n",
    "# Create a DataFrame, whatever you like to store the combined data\n",
    "data = pd.DataFrame({\"review\": reviews, \"sentiment\": labels})\n",
    "# Optionally, you can save the combined dataset to a CSV file\n",
    "data.to_csv(\"fulldata.csv\", index=False) # Replace with the desired output file path\n",
    "\n",
    "# train_data = train_pos_dir + train_neg_dir\n",
    "# test_data = test_pos_dir + test_neg_dir\n",
    "# full_data = train_data + test_data\n",
    "\n",
    "random.shuffle(full_data)\n",
    "\n",
    "num_reviews = len(full_data)\n",
    "num_train_reviews = int(num_reviews * 0.7)\n",
    "num_test_reviews = num_reviews - num_train_reviews\n",
    "\n",
    "train_reviews = full_data[:num_train_reviews]\n",
    "test_reviews = full_data[num_train_reviews:]\n",
    "\n",
    "train_data, test_data = train_test_split(full_data, test_size =0.3, random_state=42)\n",
    "\n",
    "\n",
    "positive_data = train_positive_reviews + test_positive_reviews\n",
    "\n",
    "num_positive_train = sum(1 for review in train_reviews if review in positive_data)\n",
    "num_negative_train = num_train_reviews - num_positive_train\n",
    "positive_to_negative_ratio_train = num_positive_train / num_negative_train\n",
    "\n",
    "num_positive_test = sum(1 for review in test_reviews if review in positive_data)\n",
    "num_negative_test = num_test_reviews - num_positive_test\n",
    "positive_to_negative_ratio_test = num_positive_test / num_negative_test\n",
    "\n",
    "print(f\"Training Set: Positive-to-Negative Ratio = {positive_to_negative_ratio_train:.2f}\")\n",
    "print(f\"Testing Set: Positive-to-Negative Ratio = {positive_to_negative_ratio_test:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5b1215c-8ccc-4e65-bd2d-f987c018e00a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.24.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.10.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (3.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (1.24.1)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn\n",
    "import sklearn\n",
    "%pip install pandas\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "690247da-fe1d-453a-b206-42fd163e359b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8462666666666666\n",
      "Precision (Positive): 0.8695527932561794\n",
      "Precision (Negative): 0.8258967629046369\n",
      "Recall (Positive): 0.8137451530953336\n",
      "Recall (Negative): 0.8786065682754953\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.83      0.88      0.85      7521\n",
      "         pos       0.87      0.81      0.84      7479\n",
      "\n",
      "    accuracy                           0.85     15000\n",
      "   macro avg       0.85      0.85      0.85     15000\n",
      "weighted avg       0.85      0.85      0.85     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# X_train = vectorizer.fit_transform(train_data)\n",
    "X_train = train_data\n",
    "y_train = []\n",
    "for review in train_data:\n",
    "    if review in positive_data:\n",
    "        y_train.append(\"pos\")\n",
    "    else: y_train.append(\"neg\")\n",
    "\n",
    "# X_test = vectorizer.fit_transform(test_data)\n",
    "X_test = test_data\n",
    "y_test = []\n",
    "for review in test_data:\n",
    "    if review in positive_data:\n",
    "        y_test.append(\"pos\")\n",
    "    else: y_test.append(\"neg\")\n",
    "\n",
    "# Train the pipeline using the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test) \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "precision_positive = precision_score(y_test, y_pred, pos_label=\"pos\")\n",
    "precision_negative = precision_score(y_test, y_pred, pos_label=\"neg\")\n",
    "recall_positive = recall_score(y_test, y_pred, pos_label=\"pos\")\n",
    "recall_negative = recall_score(y_test, y_pred, pos_label=\"neg\")\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision (Positive):\", precision_positive)\n",
    "print(\"Precision (Negative):\", precision_negative)\n",
    "print (\"Recall (Positive):\", recall_positive)\n",
    "print (\"Recall (Negative):\", recall_negative)\n",
    "# Generate classification report\n",
    "classification_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "710a3ae5-a43a-4df0-b292-9bf1ee4cc7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8596\n",
      "Precision (Positive): 0.8784335821946753\n",
      "Precision (Negative): 0.8426781420073408\n",
      "Recall (Positive): 0.8338013103356063\n",
      "Recall (Negative): 0.8852546203962239\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.84      0.89      0.86      7521\n",
      "         pos       0.88      0.83      0.86      7479\n",
      "\n",
      "    accuracy                           0.86     15000\n",
      "   macro avg       0.86      0.86      0.86     15000\n",
      "weighted avg       0.86      0.86      0.86     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "\n",
    "# Create a pipeline with TfidfVectorizer\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Train the pipeline using the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate accuracy..., the remaining steps remain the same\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision_positive = precision_score(y_test, y_pred, pos_label=\"pos\")\n",
    "precision_negative = precision_score(y_test, y_pred, pos_label=\"neg\")\n",
    "recall_positive = recall_score(y_test, y_pred, pos_label=\"pos\")\n",
    "recall_negative = recall_score(y_test, y_pred, pos_label=\"neg\")\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision (Positive):\", precision_positive)\n",
    "print(\"Precision (Negative):\", precision_negative)\n",
    "print (\"Recall (Positive):\", recall_positive)\n",
    "print (\"Recall (Negative):\", recall_negative)\n",
    "# Generate classification report\n",
    "classification_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a305be3b-42e1-43af-8221-3562fd53f1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8893333333333333\n",
      "Precision (Positive): 0.8852111743678008\n",
      "Precision (Negative): 0.8935141667785685\n",
      "Recall (Positive): 0.8939697820564246\n",
      "Recall (Negative): 0.8847227762265656\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.89      0.88      0.89      7521\n",
      "         pos       0.89      0.89      0.89      7479\n",
      "\n",
      "    accuracy                           0.89     15000\n",
      "   macro avg       0.89      0.89      0.89     15000\n",
      "weighted avg       0.89      0.89      0.89     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "# Create a pipeline with CountVectorizer and LogisticRegression\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Train the pipeline using the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate accuracy..., the remaining steps remain the same\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision_positive = precision_score(y_test, y_pred, pos_label=\"pos\")\n",
    "precision_negative = precision_score(y_test, y_pred, pos_label=\"neg\")\n",
    "recall_positive = recall_score(y_test, y_pred, pos_label=\"pos\")\n",
    "recall_negative = recall_score(y_test, y_pred, pos_label=\"neg\")\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision (Positive):\", precision_positive)\n",
    "print(\"Precision (Negative):\", precision_negative)\n",
    "print (\"Recall (Positive):\", recall_positive)\n",
    "print (\"Recall (Negative):\", recall_negative)\n",
    "# Generate classification report\n",
    "classification_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", classification_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8232a74-7aa8-4f28-b44c-fbff15cd284d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8943333333333333\n",
      "Precision (Positive): 0.883524206142634\n",
      "Precision (Negative): 0.905686167304538\n",
      "Recall (Positive): 0.9077416766947453\n",
      "Recall (Negative): 0.8809998670389576\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.91      0.88      0.89      7521\n",
      "         pos       0.88      0.91      0.90      7479\n",
      "\n",
      "    accuracy                           0.89     15000\n",
      "   macro avg       0.89      0.89      0.89     15000\n",
      "weighted avg       0.89      0.89      0.89     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "# Create a pipeline with CountVectorizer and LogisticRegression\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Train the pipeline using the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate accuracy..., the remaining steps remain the same\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision_positive = precision_score(y_test, y_pred, pos_label=\"pos\")\n",
    "precision_negative = precision_score(y_test, y_pred, pos_label=\"neg\")\n",
    "recall_positive = recall_score(y_test, y_pred, pos_label=\"pos\")\n",
    "recall_negative = recall_score(y_test, y_pred, pos_label=\"neg\")\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision (Positive):\", precision_positive)\n",
    "print(\"Precision (Negative):\", precision_negative)\n",
    "print (\"Recall (Positive):\", recall_positive)\n",
    "print (\"Recall (Negative):\", recall_negative)\n",
    "# Generate classification report\n",
    "classification_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d24ae8fc-d716-4432-83f5-e709c2d1103e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8960666666666667\n",
      "Precision (Positive): 0.8839169909208819\n",
      "Precision (Negative): 0.9089163237311385\n",
      "Recall (Positive): 0.9112180772830593\n",
      "Recall (Negative): 0.8809998670389576\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.91      0.88      0.89      7521\n",
      "         pos       0.88      0.91      0.90      7479\n",
      "\n",
      "    accuracy                           0.90     15000\n",
      "   macro avg       0.90      0.90      0.90     15000\n",
      "weighted avg       0.90      0.90      0.90     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "# Create a pipeline with TfidfVectorizer and LogisticRegression\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(ngram_range=(1,2))),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "# Train the pipeline using the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate accuracy..., the remaining steps remain the same\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision_positive = precision_score(y_test, y_pred, pos_label=\"pos\")\n",
    "precision_negative = precision_score(y_test, y_pred, pos_label=\"neg\")\n",
    "recall_positive = recall_score(y_test, y_pred, pos_label=\"pos\")\n",
    "recall_negative = recall_score(y_test, y_pred, pos_label=\"neg\")\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision (Positive):\", precision_positive)\n",
    "print(\"Precision (Negative):\", precision_negative)\n",
    "print (\"Recall (Positive):\", recall_positive)\n",
    "print (\"Recall (Negative):\", recall_negative)\n",
    "# Generate classification report\n",
    "classification_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a13b039a-cdc7-4489-958a-4ae731b8d848",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9046\n",
      "Precision (Positive): 0.8946750195771339\n",
      "Precision (Negative): 0.9149632052330335\n",
      "Recall (Positive): 0.9165663858804654\n",
      "Recall (Negative): 0.89270043877144\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.91      0.89      0.90      7521\n",
      "         pos       0.89      0.92      0.91      7479\n",
      "\n",
      "    accuracy                           0.90     15000\n",
      "   macro avg       0.90      0.90      0.90     15000\n",
      "weighted avg       0.90      0.90      0.90     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "# Create a pipeline with CountVectorizer and LogisticRegression\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(ngram_range=(1,2))),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Train the pipeline using the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate accuracy..., the remaining steps remain the same\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision_positive = precision_score(y_test, y_pred, pos_label=\"pos\")\n",
    "precision_negative = precision_score(y_test, y_pred, pos_label=\"neg\")\n",
    "recall_positive = recall_score(y_test, y_pred, pos_label=\"pos\")\n",
    "recall_negative = recall_score(y_test, y_pred, pos_label=\"neg\")\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision (Positive):\", precision_positive)\n",
    "print(\"Precision (Negative):\", precision_negative)\n",
    "print (\"Recall (Positive):\", recall_positive)\n",
    "print (\"Recall (Negative):\", recall_negative)\n",
    "# Generate classification report\n",
    "classification_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3c57d11-337e-426f-85a7-a104a5282012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8834\n",
      "Precision (Positive): 0.904775360271263\n",
      "Precision (Negative): 0.8643019439535471\n",
      "Recall (Positive): 0.8562642064447119\n",
      "Recall (Negative): 0.9103842574125781\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.86      0.91      0.89      7521\n",
      "         pos       0.90      0.86      0.88      7479\n",
      "\n",
      "    accuracy                           0.88     15000\n",
      "   macro avg       0.88      0.88      0.88     15000\n",
      "weighted avg       0.88      0.88      0.88     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "\n",
    "# Create a pipeline with TfidfVectorizer\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(ngram_range=(1,2))),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Train the pipeline using the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate accuracy..., the remaining steps remain the same\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision_positive = precision_score(y_test, y_pred, pos_label=\"pos\")\n",
    "precision_negative = precision_score(y_test, y_pred, pos_label=\"neg\")\n",
    "recall_positive = recall_score(y_test, y_pred, pos_label=\"pos\")\n",
    "recall_negative = recall_score(y_test, y_pred, pos_label=\"neg\")\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision (Positive):\", precision_positive)\n",
    "print(\"Precision (Negative):\", precision_negative)\n",
    "print (\"Recall (Positive):\", recall_positive)\n",
    "print (\"Recall (Negative):\", recall_negative)\n",
    "# Generate classification report\n",
    "classification_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae00d604-75ee-4917-abfe-eaf74dbf2042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8784\n",
      "Precision (Positive): 0.8921092774927195\n",
      "Precision (Negative): 0.8657080498138401\n",
      "Recall (Positive): 0.8601417301778312\n",
      "Recall (Negative): 0.8965563090014625\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.87      0.90      0.88      7521\n",
      "         pos       0.89      0.86      0.88      7479\n",
      "\n",
      "    accuracy                           0.88     15000\n",
      "   macro avg       0.88      0.88      0.88     15000\n",
      "weighted avg       0.88      0.88      0.88     15000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(ngram_range=(1,2))),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Train the pipeline using the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipeline.predict(X_test) \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "precision_positive = precision_score(y_test, y_pred, pos_label=\"pos\")\n",
    "precision_negative = precision_score(y_test, y_pred, pos_label=\"neg\")\n",
    "recall_positive = recall_score(y_test, y_pred, pos_label=\"pos\")\n",
    "recall_negative = recall_score(y_test, y_pred, pos_label=\"neg\")\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision (Positive):\", precision_positive)\n",
    "print(\"Precision (Negative):\", precision_negative)\n",
    "print (\"Recall (Positive):\", recall_positive)\n",
    "print (\"Recall (Negative):\", recall_negative)\n",
    "# Generate classification report\n",
    "classification_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fca421a4-9738-42cf-86dc-19d7e0f93a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline \n",
    "\n",
    "# Create a pipeline with TfidfVectorizer and LogisticRegression\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(ngram_range=(1,2))),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "# Train the pipeline using the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# best_medel = y_pred\n",
    "\n",
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump(pipeline, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba7473b-b5f3-417b-ab41-371c043864cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install fasttext\n",
    "import fasttext\n",
    "\n",
    "# print(train_data)\n",
    "\n",
    "model = fasttext.train_supervised(input='train.txt')\n",
    "y_pred = model.predict(X_test)[0]\n",
    "\n",
    "y_pred = [label[0].replace('__label__', '') for label in y_pred]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision_positive = precision_score(y_test, y_pred, pos_label='positive')\n",
    "precision_negative = precision_score(y_test, y_pred, pos_label='negative')\n",
    "recall_positive = recall_score(y_test, y_pred, pos_label='positive')\n",
    "recall_negative = recall_score(y_test, y_pred, pos_label='negative')\n",
    "classification_report = classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dfce6cd-6204-4bae-9780-8aab37d0f81b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLossySetitemError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:5218\u001b[0m, in \u001b[0;36mIndex._validate_fill_value\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5217\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 5218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp_can_hold_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m LossySetitemError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   5220\u001b[0m     \u001b[38;5;66;03m# re-raise as TypeError for consistency\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/dtypes/cast.py:2009\u001b[0m, in \u001b[0;36mnp_can_hold_element\u001b[0;34m(dtype, element)\u001b[0m\n\u001b[1;32m   2007\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m element\n\u001b[0;32m-> 2009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LossySetitemError\n\u001b[1;32m   2011\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mLossySetitemError\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:6870\u001b[0m, in \u001b[0;36mIndex.insert\u001b[0;34m(self, loc, item)\u001b[0m\n\u001b[1;32m   6869\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 6870\u001b[0m         item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_fill_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6871\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m, LossySetitemError):\n\u001b[1;32m   6872\u001b[0m     \u001b[38;5;66;03m# e.g. trying to insert an integer into a DatetimeIndex\u001b[39;00m\n\u001b[1;32m   6873\u001b[0m     \u001b[38;5;66;03m#  We cannot keep the same dtype, so cast to the (often object)\u001b[39;00m\n\u001b[1;32m   6874\u001b[0m     \u001b[38;5;66;03m#  minimal shared dtype before doing the insert.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:5221\u001b[0m, in \u001b[0;36mIndex._validate_fill_value\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5219\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m LossySetitemError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   5220\u001b[0m         \u001b[38;5;66;03m# re-raise as TypeError for consistency\u001b[39;00m\n\u001b[0;32m-> 5221\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   5222\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m can_hold_element(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values, value):\n",
      "\u001b[0;31mTypeError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m train_data:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m review \u001b[38;5;129;01min\u001b[39;00m positive_data:\n\u001b[0;32m----> 6\u001b[0m         \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: df\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/frame.py:4822\u001b[0m, in \u001b[0;36mDataFrame.insert\u001b[0;34m(self, loc, column, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   4819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloc must be int\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4821\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sanitize_column(value)\n\u001b[0;32m-> 4822\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/internals/managers.py:1405\u001b[0m, in \u001b[0;36mBlockManager.insert\u001b[0;34m(self, loc, item, value)\u001b[0m\n\u001b[1;32m   1395\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;124;03mInsert item at selected position.\u001b[39;00m\n\u001b[1;32m   1397\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;124;03mvalue : np.ndarray or ExtensionArray\u001b[39;00m\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1404\u001b[0m \u001b[38;5;66;03m# insert to the axis; this could possibly raise a TypeError\u001b[39;00m\n\u001b[0;32m-> 1405\u001b[0m new_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1408\u001b[0m     value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/range.py:885\u001b[0m, in \u001b[0;36mRangeIndex.insert\u001b[0;34m(self, loc, item)\u001b[0m\n\u001b[1;32m    882\u001b[0m         new_rng \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop, step)\n\u001b[1;32m    883\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_simple_new(new_rng, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m--> 885\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:6876\u001b[0m, in \u001b[0;36mIndex.insert\u001b[0;34m(self, loc, item)\u001b[0m\n\u001b[1;32m   6871\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m, LossySetitemError):\n\u001b[1;32m   6872\u001b[0m     \u001b[38;5;66;03m# e.g. trying to insert an integer into a DatetimeIndex\u001b[39;00m\n\u001b[1;32m   6873\u001b[0m     \u001b[38;5;66;03m#  We cannot keep the same dtype, so cast to the (often object)\u001b[39;00m\n\u001b[1;32m   6874\u001b[0m     \u001b[38;5;66;03m#  minimal shared dtype before doing the insert.\u001b[39;00m\n\u001b[1;32m   6875\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_common_type_compat(item)\n\u001b[0;32m-> 6876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   6879\u001b[0m     item, (\u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mdatetime64, np\u001b[38;5;241m.\u001b[39mtimedelta64)\n\u001b[1;32m   6880\u001b[0m ):\n\u001b[1;32m   6881\u001b[0m     \u001b[38;5;66;03m# with object-dtype we need to worry about numpy incorrectly casting\u001b[39;00m\n\u001b[1;32m   6882\u001b[0m     \u001b[38;5;66;03m# dt64/td64 to integer, also about treating tuples as sequences\u001b[39;00m\n\u001b[1;32m   6883\u001b[0m     \u001b[38;5;66;03m# special-casing dt64/td64 https://github.com/numpy/numpy/issues/12550\u001b[39;00m\n\u001b[1;32m   6884\u001b[0m     casted \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype(item)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:6885\u001b[0m, in \u001b[0;36mIndex.insert\u001b[0;34m(self, loc, item)\u001b[0m\n\u001b[1;32m   6878\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   6879\u001b[0m     item, (\u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mdatetime64, np\u001b[38;5;241m.\u001b[39mtimedelta64)\n\u001b[1;32m   6880\u001b[0m ):\n\u001b[1;32m   6881\u001b[0m     \u001b[38;5;66;03m# with object-dtype we need to worry about numpy incorrectly casting\u001b[39;00m\n\u001b[1;32m   6882\u001b[0m     \u001b[38;5;66;03m# dt64/td64 to integer, also about treating tuples as sequences\u001b[39;00m\n\u001b[1;32m   6883\u001b[0m     \u001b[38;5;66;03m# special-casing dt64/td64 https://github.com/numpy/numpy/issues/12550\u001b[39;00m\n\u001b[1;32m   6884\u001b[0m     casted \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype(item)\n\u001b[0;32m-> 6885\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6887\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6888\u001b[0m     \u001b[38;5;66;03m# error: No overload variant of \"insert\" matches argument types\u001b[39;00m\n\u001b[1;32m   6889\u001b[0m     \u001b[38;5;66;03m# \"ndarray[Any, Any]\", \"int\", \"None\"\u001b[39;00m\n\u001b[1;32m   6890\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39minsert(arr, loc, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36minsert\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/lib/function_base.py:5387\u001b[0m, in \u001b[0;36minsert\u001b[0;34m(arr, obj, values, axis)\u001b[0m\n\u001b[1;32m   5385\u001b[0m index \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m   5386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mN \u001b[38;5;129;01mor\u001b[39;00m index \u001b[38;5;241m>\u001b[39m N:\n\u001b[0;32m-> 5387\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is out of bounds for axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5388\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mN\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (index \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m   5390\u001b[0m     index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m N\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# df = pd.DataFrame(train_data)\n",
    "# for review in train_data:\n",
    "#     if review in positive_data:\n",
    "#         df.insert(2,\"label\",\"pos\")\n",
    "#     else: df.insert(2,\"label\",\"neg\")\n",
    "# print(df)\n",
    "\n",
    "# path = r'/Users/shun/Desktop/IERG4080/train.txt'\n",
    "# with open(path, 'a') as f:\n",
    "#     df = df.to_string(header=False, index=False)\n",
    "#     f.write(df)\n",
    "\n",
    "# for filename in os.listdir(train_data):\n",
    "#     with open(os.path.join(train_data, filename), \"r\", encoding=\"utf-8\") as file:\n",
    "#         review = file.read()\n",
    "#         reviews.append(review)\n",
    "#         if review in positive_data:\n",
    "#             labels.append(\"pos\")\n",
    "#         else: labels.append(\"neg\")\n",
    "\n",
    "# # Create a DataFrame, whatever you like to store the combined data\n",
    "# data = pd.DataFrame({\"review\": reviews, \"sentiment\": labels})\n",
    "# # Optionally, you can save the combined dataset to a CSV file\n",
    "# data.to_csv(\"traindata.csv\", index=False) # Replace with the desired output file path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2545936-7b90-40e2-9081-20d175eb052e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
